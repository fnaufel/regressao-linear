[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regressão Linear",
    "section": "",
    "text": "Apresentação\n???",
    "crumbs": [
      "Apresentação"
    ]
  },
  {
    "objectID": "simples.html",
    "href": "simples.html",
    "title": "\n1  Regressão linear simples\n",
    "section": "",
    "text": "1.1 Exemplo: vendas e publicidade\nExemplo baseado no livro James et al. (2021), com dados obtidos de https://www.kaggle.com/datasets/ashydv/advertising-dataset/data.\nEste conjunto de dados contém \\(4\\) colunas:\nCada observação — isto é, cada linha — corresponde a um produto.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regressão linear simples</span>"
    ]
  },
  {
    "objectID": "simples.html#exemplo-vendas-e-publicidade",
    "href": "simples.html#exemplo-vendas-e-publicidade",
    "title": "\n1  Regressão linear simples\n",
    "section": "",
    "text": "tv: verba (em milhares de dólares) gasta em publicidade na TV;\n\nradio: verba (em milhares de dólares) gasta em publicidade no rádio;\n\njornal: verba (em milhares de dólares) gasta em publicidade em jornais;\n\nvendas: receita das vendas (em milhares de dólares).\n\n\nLeitura e limpeza\n\npublicidade &lt;- read_csv(\n  'dados/advertising.csv',\n  show_col_types = FALSE\n) %&gt;% \n  janitor::clean_names() %&gt;% \n  rename(\n    jornal = newspaper,\n    vendas = sales\n  )\n\npublicidade %&gt;% gt()\n\n\n\n\n\n\ntv\nradio\njornal\nvendas\n\n\n\n230,1\n37,8\n69,2\n22,1\n\n\n44,5\n39,3\n45,1\n10,4\n\n\n17,2\n45,9\n69,3\n12,0\n\n\n151,5\n41,3\n58,5\n16,5\n\n\n180,8\n10,8\n58,4\n17,9\n\n\n8,7\n48,9\n75,0\n7,2\n\n\n57,5\n32,8\n23,5\n11,8\n\n\n120,2\n19,6\n11,6\n13,2\n\n\n8,6\n2,1\n1,0\n4,8\n\n\n199,8\n2,6\n21,2\n15,6\n\n\n66,1\n5,8\n24,2\n12,6\n\n\n214,7\n24,0\n4,0\n17,4\n\n\n23,8\n35,1\n65,9\n9,2\n\n\n97,5\n7,6\n7,2\n13,7\n\n\n204,1\n32,9\n46,0\n19,0\n\n\n195,4\n47,7\n52,9\n22,4\n\n\n67,8\n36,6\n114,0\n12,5\n\n\n281,4\n39,6\n55,8\n24,4\n\n\n69,2\n20,5\n18,3\n11,3\n\n\n147,3\n23,9\n19,1\n14,6\n\n\n218,4\n27,7\n53,4\n18,0\n\n\n237,4\n5,1\n23,5\n17,5\n\n\n13,2\n15,9\n49,6\n5,6\n\n\n228,3\n16,9\n26,2\n20,5\n\n\n62,3\n12,6\n18,3\n9,7\n\n\n262,9\n3,5\n19,5\n17,0\n\n\n142,9\n29,3\n12,6\n15,0\n\n\n240,1\n16,7\n22,9\n20,9\n\n\n248,8\n27,1\n22,9\n18,9\n\n\n70,6\n16,0\n40,8\n10,5\n\n\n292,9\n28,3\n43,2\n21,4\n\n\n112,9\n17,4\n38,6\n11,9\n\n\n97,2\n1,5\n30,0\n13,2\n\n\n265,6\n20,0\n0,3\n17,4\n\n\n95,7\n1,4\n7,4\n11,9\n\n\n290,7\n4,1\n8,5\n17,8\n\n\n266,9\n43,8\n5,0\n25,4\n\n\n74,7\n49,4\n45,7\n14,7\n\n\n43,1\n26,7\n35,1\n10,1\n\n\n228,0\n37,7\n32,0\n21,5\n\n\n202,5\n22,3\n31,6\n16,6\n\n\n177,0\n33,4\n38,7\n17,1\n\n\n293,6\n27,7\n1,8\n20,7\n\n\n206,9\n8,4\n26,4\n17,9\n\n\n25,1\n25,7\n43,3\n8,5\n\n\n175,1\n22,5\n31,5\n16,1\n\n\n89,7\n9,9\n35,7\n10,6\n\n\n239,9\n41,5\n18,5\n23,2\n\n\n227,2\n15,8\n49,9\n19,8\n\n\n66,9\n11,7\n36,8\n9,7\n\n\n199,8\n3,1\n34,6\n16,4\n\n\n100,4\n9,6\n3,6\n10,7\n\n\n216,4\n41,7\n39,6\n22,6\n\n\n182,6\n46,2\n58,7\n21,2\n\n\n262,7\n28,8\n15,9\n20,2\n\n\n198,9\n49,4\n60,0\n23,7\n\n\n7,3\n28,1\n41,4\n5,5\n\n\n136,2\n19,2\n16,6\n13,2\n\n\n210,8\n49,6\n37,7\n23,8\n\n\n210,7\n29,5\n9,3\n18,4\n\n\n53,5\n2,0\n21,4\n8,1\n\n\n261,3\n42,7\n54,7\n24,2\n\n\n239,3\n15,5\n27,3\n20,7\n\n\n102,7\n29,6\n8,4\n14,0\n\n\n131,1\n42,8\n28,9\n16,0\n\n\n69,0\n9,3\n0,9\n11,3\n\n\n31,5\n24,6\n2,2\n11,0\n\n\n139,3\n14,5\n10,2\n13,4\n\n\n237,4\n27,5\n11,0\n18,9\n\n\n216,8\n43,9\n27,2\n22,3\n\n\n199,1\n30,6\n38,7\n18,3\n\n\n109,8\n14,3\n31,7\n12,4\n\n\n26,8\n33,0\n19,3\n8,8\n\n\n129,4\n5,7\n31,3\n11,0\n\n\n213,4\n24,6\n13,1\n17,0\n\n\n16,9\n43,7\n89,4\n8,7\n\n\n27,5\n1,6\n20,7\n6,9\n\n\n120,5\n28,5\n14,2\n14,2\n\n\n5,4\n29,9\n9,4\n5,3\n\n\n116,0\n7,7\n23,1\n11,0\n\n\n76,4\n26,7\n22,3\n11,8\n\n\n239,8\n4,1\n36,9\n17,3\n\n\n75,3\n20,3\n32,5\n11,3\n\n\n68,4\n44,5\n35,6\n13,6\n\n\n213,5\n43,0\n33,8\n21,7\n\n\n193,2\n18,4\n65,7\n20,2\n\n\n76,3\n27,5\n16,0\n12,0\n\n\n110,7\n40,6\n63,2\n16,0\n\n\n88,3\n25,5\n73,4\n12,9\n\n\n109,8\n47,8\n51,4\n16,7\n\n\n134,3\n4,9\n9,3\n14,0\n\n\n28,6\n1,5\n33,0\n7,3\n\n\n217,7\n33,5\n59,0\n19,4\n\n\n250,9\n36,5\n72,3\n22,2\n\n\n107,4\n14,0\n10,9\n11,5\n\n\n163,3\n31,6\n52,9\n16,9\n\n\n197,6\n3,5\n5,9\n16,7\n\n\n184,9\n21,0\n22,0\n20,5\n\n\n289,7\n42,3\n51,2\n25,4\n\n\n135,2\n41,7\n45,9\n17,2\n\n\n222,4\n4,3\n49,8\n16,7\n\n\n296,4\n36,3\n100,9\n23,8\n\n\n280,2\n10,1\n21,4\n19,8\n\n\n187,9\n17,2\n17,9\n19,7\n\n\n238,2\n34,3\n5,3\n20,7\n\n\n137,9\n46,4\n59,0\n15,0\n\n\n25,0\n11,0\n29,7\n7,2\n\n\n90,4\n0,3\n23,2\n12,0\n\n\n13,1\n0,4\n25,6\n5,3\n\n\n255,4\n26,9\n5,5\n19,8\n\n\n225,8\n8,2\n56,5\n18,4\n\n\n241,7\n38,0\n23,2\n21,8\n\n\n175,7\n15,4\n2,4\n17,1\n\n\n209,6\n20,6\n10,7\n20,9\n\n\n78,2\n46,8\n34,5\n14,6\n\n\n75,1\n35,0\n52,7\n12,6\n\n\n139,2\n14,3\n25,6\n12,2\n\n\n76,4\n0,8\n14,8\n9,4\n\n\n125,7\n36,9\n79,2\n15,9\n\n\n19,4\n16,0\n22,3\n6,6\n\n\n141,3\n26,8\n46,2\n15,5\n\n\n18,8\n21,7\n50,4\n7,0\n\n\n224,0\n2,4\n15,6\n16,6\n\n\n123,1\n34,6\n12,4\n15,2\n\n\n229,5\n32,3\n74,2\n19,7\n\n\n87,2\n11,8\n25,9\n10,6\n\n\n7,8\n38,9\n50,6\n6,6\n\n\n80,2\n0,0\n9,2\n11,9\n\n\n220,3\n49,0\n3,2\n24,7\n\n\n59,6\n12,0\n43,1\n9,7\n\n\n0,7\n39,6\n8,7\n1,6\n\n\n265,2\n2,9\n43,0\n17,7\n\n\n8,4\n27,2\n2,1\n5,7\n\n\n219,8\n33,5\n45,1\n19,6\n\n\n36,9\n38,6\n65,6\n10,8\n\n\n48,3\n47,0\n8,5\n11,6\n\n\n25,6\n39,0\n9,3\n9,5\n\n\n273,7\n28,9\n59,7\n20,8\n\n\n43,0\n25,9\n20,5\n9,6\n\n\n184,9\n43,9\n1,7\n20,7\n\n\n73,4\n17,0\n12,9\n10,9\n\n\n193,7\n35,4\n75,6\n19,2\n\n\n220,5\n33,2\n37,9\n20,1\n\n\n104,6\n5,7\n34,4\n10,4\n\n\n96,2\n14,8\n38,9\n12,3\n\n\n140,3\n1,9\n9,0\n10,3\n\n\n240,1\n7,3\n8,7\n18,2\n\n\n243,2\n49,0\n44,3\n25,4\n\n\n38,0\n40,3\n11,9\n10,9\n\n\n44,7\n25,8\n20,6\n10,1\n\n\n280,7\n13,9\n37,0\n16,1\n\n\n121,0\n8,4\n48,7\n11,6\n\n\n197,6\n23,3\n14,2\n16,6\n\n\n171,3\n39,7\n37,7\n16,0\n\n\n187,8\n21,1\n9,5\n20,6\n\n\n4,1\n11,6\n5,7\n3,2\n\n\n93,9\n43,5\n50,5\n15,3\n\n\n149,8\n1,3\n24,3\n10,1\n\n\n11,7\n36,9\n45,2\n7,3\n\n\n131,7\n18,4\n34,6\n12,9\n\n\n172,5\n18,1\n30,7\n16,4\n\n\n85,7\n35,8\n49,3\n13,3\n\n\n188,4\n18,1\n25,6\n19,9\n\n\n163,5\n36,8\n7,4\n18,0\n\n\n117,2\n14,7\n5,4\n11,9\n\n\n234,5\n3,4\n84,8\n16,9\n\n\n17,9\n37,6\n21,6\n8,0\n\n\n206,8\n5,2\n19,4\n17,2\n\n\n215,4\n23,6\n57,6\n17,1\n\n\n284,3\n10,6\n6,4\n20,0\n\n\n50,0\n11,6\n18,4\n8,4\n\n\n164,5\n20,9\n47,4\n17,5\n\n\n19,6\n20,1\n17,0\n7,6\n\n\n168,4\n7,1\n12,8\n16,7\n\n\n222,4\n3,4\n13,1\n16,5\n\n\n276,9\n48,9\n41,8\n27,0\n\n\n248,4\n30,2\n20,3\n20,2\n\n\n170,2\n7,8\n35,2\n16,7\n\n\n276,7\n2,3\n23,7\n16,8\n\n\n165,6\n10,0\n17,6\n17,6\n\n\n156,6\n2,6\n8,3\n15,5\n\n\n218,5\n5,4\n27,4\n17,2\n\n\n56,2\n5,7\n29,7\n8,7\n\n\n287,6\n43,0\n71,8\n26,2\n\n\n253,8\n21,3\n30,0\n17,6\n\n\n205,0\n45,1\n19,6\n22,6\n\n\n139,5\n2,1\n26,6\n10,3\n\n\n191,1\n28,7\n18,2\n17,3\n\n\n286,0\n13,9\n3,7\n20,9\n\n\n18,7\n12,1\n23,4\n6,7\n\n\n39,5\n41,1\n5,8\n10,8\n\n\n75,5\n10,8\n6,0\n11,9\n\n\n17,2\n4,1\n31,6\n5,9\n\n\n166,8\n42,0\n3,6\n19,6\n\n\n149,7\n35,6\n6,0\n17,3\n\n\n38,2\n3,7\n13,8\n7,6\n\n\n94,2\n4,9\n8,1\n14,0\n\n\n177,0\n9,3\n6,4\n14,8\n\n\n283,6\n42,0\n66,2\n25,5\n\n\n232,1\n8,6\n8,7\n18,4\n\n\n\n\n\n\n\nDivisão em dados de treino e teste\n\nsplit &lt;- initial_split(publicidade)\ntreino &lt;- training(split)\nteste &lt;- testing(split)\nsplit\n\n&lt;Training/Testing/Total&gt;\n&lt;150/50/200&gt;\n\n\nVendas por verba gasta em TV\nAnálise exploratória\nComeçamos visualizando os dados:\n\ngrafico &lt;- treino %&gt;% \n  ggplot(aes(tv, vendas)) +\n    geom_point()\n\ngrafico\n\n\n\n\n\n\n\nA correlação linear entre vendas e tv é\n\ncor(treino$vendas, treino$tv)\n\n[1] 0,8926062\n\n\nModelo linear\n\nmodelo &lt;- lm(vendas ~ tv, data = treino)\nsummary(modelo)\n\n\nCall:\nlm(formula = vendas ~ tv, data = treino)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6,3046 -1,7075 -0,0832  1,7818  5,7983 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(&gt;|t|)    \n(Intercept) 6,952703   0,386109   18,01 &lt;0,0000000000000002 ***\ntv          0,055048   0,002285   24,09 &lt;0,0000000000000002 ***\n---\nSignif. codes:  0 '***' 0,001 '**' 0,01 '*' 0,05 '.' 0,1 ' ' 1\n\nResidual standard error: 2,402 on 148 degrees of freedom\nMultiple R-squared:  0,7967,    Adjusted R-squared:  0,7954 \nF-statistic: 580,2 on 1 and 148 DF,  p-value: &lt; 0,00000000000000022\n\n\n\nmodelo_tidy &lt;- tidy(modelo)\nmodelo_tidy\n\n\n  \n\n\n\n\nb0 &lt;- modelo_tidy$estimate[1]\nb1 &lt;- modelo_tidy$estimate[2]\n\n\ngrafico +\n  geom_abline(\n    intercept = b0,\n    slope = b1,\n    color = 'blue'\n  )\n\n\n\n\n\n\n\nA equação da reta é\n\\[\n\\begin{aligned}\n  \\widehat{\\text{vendas}}\n  &= \\hat{\\beta_0} + \\hat{\\beta_1} \\cdot \\text{tv} \\\\\n  &= 6{,}95 + 0{,}06 \\cdot \\text{tv}\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regressão linear simples</span>"
    ]
  },
  {
    "objectID": "simples.html#teoria",
    "href": "simples.html#teoria",
    "title": "\n1  Regressão linear simples\n",
    "section": "\n1.2 Teoria",
    "text": "1.2 Teoria\nEstimativas \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\)\n\nOs valores achados são estimativas para \\(\\beta_0\\) e \\(\\beta_1\\), baseadas nos dados do conjunto de treino.\nPor isso, os valores de vendas obtidos com esta equação também são estimativas.\nVamos escrever estimativas com o acento circunflexo (chapéu) sobre os símbolos.\nDe onde vêm os valores de \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\)?\nResposta: são os valores que fazem com que a soma dos quadrados das distâncias verticais dos pontos à reta seja a menor possível.\n(Estas distâncias são chamadas de resíduos.)\nConsulte este material para ver os detalhes sobre o cálculo de \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\).\nErros-padrão das estimativas\nVamos pensar nas incertezas associadas aos valores de \\(\\hat{\\beta_0}\\) e \\(\\hat{\\beta_1}\\), com base na excelente discussão em (De Veaux, Velleman e Bock 2016, cap. 25).\nQuais são os fatores que afetam a nossa confiança na reta de regressão?\nMais especificamente, quais os fatores que afetam nossa confiança no valor estimado \\(\\hat\\beta_1\\) (a inclinação da reta)?\nEspalhamento dos pontos em volta da reta\nQuanto mais afastados da reta estiverem os dados, menor a nossa confiança de que a reta captura a variação de uma variável em função da outra.\nObserve a Figura 1.1. O gráfico da esquerda nos dá mais certeza de que uma reta de regressão terá uma inclinação bem próxima da taxa de variação de \\(y\\) em função de \\(x\\) na população.\n\n\n\n\n\nFigura 1.1: Espalhamento dos pontos\n\n\nEste espalhamento é medido pelo desvio-padrão dos resíduos.\nNo exemplo das vendas, este desvio-padrão dos resíduos é calculado como\n\\[\n\\displaystyle\n\\sqrt{\n\\frac{\\sum_i (\\text{vendas}_i - \\widehat{\\text{vendas}}_i)^2}{n-2}\n}\n\\]\nNo numerador, o valor \\(\\text{vendas}_i - \\widehat{\\text{vendas}}_i\\) é o resíduo da observação \\(i\\).\nAs vendas estimadas para cada valor de tv e os valores dos resíduos podem ser acessados assim:\n\nmodelo_augment &lt;- augment(modelo)\nmodelo_augment %&gt;% \n  select(vendas, tv, .fitted, .resid)\n\n\n  \n\n\n\nCalculando o desvio-padrão dos resíduos:\n\nn &lt;- nrow(modelo_augment)\ndp_residuos &lt;- sqrt(sum(modelo_augment$.resid^2) / (n - 2))\ndp_residuos\n\n[1] 2,402205\n\n\nEste valor pode ser obtido na coluna sigma do data frame retornado pela função glance:\n\nmodelo_glance &lt;- glance(modelo)\nmodelo_glance$sigma\n\n[1] 2,402205\n\n\n\n\n\n\n\n\nDesvio-padrão dos resíduos\n\n\n\nNo geral, então, em uma regressão da variável \\(y\\) sobre a variável \\(x\\) com \\(n\\) observações, o desvio-padrão dos resíduos é\n\\[\n\\displaystyle\ns_{\\text{residuos}} =\n\\sqrt{\n\\frac{\\sum_i (y_i - \\widehat{y}_i)^2}{n-2}\n}\n\\]\nPela Figura 1.1 e pelos comentários acima, quanto maior o valor de \\(s_{\\text{residuos}}\\), maior a nossa incerteza.\n\n\nEspalhamento de \\(x\\)\n\nQuanto maior o espalhamento dos valores de \\(x\\), maior nossa confiança na reta de regressão, pois ela estará baseada em uma diversidade maior de valores.\nObserve a Figura 1.2. O gráfico da direita tem um espalhamento maior dos valores de \\(x\\). Uma reta de regressão, ali, parece estar mais bem “ancorada”.\n\n\n\n\n\nFigura 1.2: Espalhamento de \\(x\\)\n\n\nO espalhamento de \\(x\\) é medido pelo desvio-padrão, que é calculado da maneira usual.\nNo exemplo das vendas, \\(s_x\\), o desvio-padrão de tv é\n\ndp_x &lt;- modelo_augment %&gt;% \n  pull(tv) %&gt;% \n  sd()\n\ndp_x\n\n[1] 86,10888\n\n\n\n\n\n\n\n\nDesvio-padrão dos resíduos\n\n\n\nPela Figura 1.2 e pelos comentários acima, quanto maior o valor de \\(s_x\\), menor a nossa incerteza.\n\n\nQuantidade de dados\nUma reta baseada em mais pontos é mais confiável. Observe a Figura 1.3.\n\n\n\n\n\nFigura 1.3: Quantidade de dados\n\n\n\n\n\n\n\n\nQuantidade de dados\n\n\n\nPela Figura 1.3 e pelos comentários acima, quanto maior o valor de \\(n\\), menor a nossa incerteza.\n\n\nJuntando tudo\nVimos que\n\nQuanto maior o desvio-padrão dos resíduos (\\(s_{\\text{residuos}}\\)), maior a incerteza.\nQuanto maior o desvio-padrão da variável \\(x\\) (\\(s_x\\)), menor a incerteza.\nQuanto maior a quantidade de dados (\\(n\\)), menor a incerteza.\n\nConcluímos que a incerteza sobre nossa estimativa para \\(\\beta_1\\) (a inclinação da reta) é proporcional aos valores acima da seguinte maneira:\n\\[\nEP(\\beta_1) \\propto \\frac{s_{\\text{residuos}}}{n \\cdot s_x}\n\\]\nonde estamos escrevendo a incerteza como \\(EP(\\beta_1)\\), o erro-padrão de \\(\\beta_1\\).\n\n\n\n\n\n\nErro-padrão de \\(\\beta_1\\)\n\n\n\nA fórmula exata para a incerteza sobre \\(\\beta_1\\) é\n\\[\nEP(\\beta_1) = \\frac{s_{\\text{residuos}}}{\\sqrt{n - 1} \\cdot s_x}\n\\]\n\n\nNo exemplo das vendas, usando as variáveis que já calculamos antes, este erro-padrão é\n\ndp_residuos / (sqrt(n - 1) * dp_x)\n\n[1] 0,002285436\n\n\nEste valor aparece nos resultados de lm como std.error:\n\nmodelo_tidy\n\n\n  \n\n\n\nErro-padrão do intercepto\n\n\n\n\n\n\nErro-padrão de \\(\\beta_0\\)\n\n\n\nPara o intercepto \\(\\beta_0\\), o raciocínio é análogo.\nA fórmula exata para a incerteza sobre \\(\\beta_0\\) é\n\\[\nEP(\\beta_0) =\n\\]\n\n\n??? ISLR p. 76",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regressão linear simples</span>"
    ]
  },
  {
    "objectID": "simples.html#visão-geométrica",
    "href": "simples.html#visão-geométrica",
    "title": "\n1  Regressão linear simples\n",
    "section": "\n1.3 Visão geométrica",
    "text": "1.3 Visão geométrica\nFaraway (2016)\nUm pequeno exemplo\nPara podermos visualizar a geometria, vamos considerar um conjunto de dados com apenas \\(3\\) observações.\nA variável x é o único preditor, e a variável y é a resposta.\n\ndf &lt;- tibble(\n  x = 1:3,\n  y = c(4, 3, 8)\n)\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n1\n4\n\n\n2\n3\n\n\n3\n8\n\n\n\n\n\n\n\nGraficamente:\n\n\n\n\n\n\n\n\nCom um único preditor, este é um exemplo de regressão simples. Queremos achar uma equação da forma\n\\[\n\\hat y = \\beta_0 + \\beta_1x\n\\]\ncom valores de \\(\\beta_0\\) e \\(\\beta_1\\) que garantam a menor soma dos quadrados dos resíduos.\nUsamos o R para achar os coeficientes e outras informações sobre este modelo:\n\nmodelo &lt;- lm(y ~ x, df)\nsummary(modelo)\n\n\nCall:\nlm(formula = y ~ x, data = df)\n\nResiduals:\n 1  2  3 \n 1 -2  1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)    1,000      3,742   0,267    0,834\nx              2,000      1,732   1,155    0,454\n\nResidual standard error: 2,449 on 1 degrees of freedom\nMultiple R-squared:  0,5714,    Adjusted R-squared:  0,1429 \nF-statistic: 1,333 on 1 and 1 DF,  p-value: 0,4544\n\n\nA equação da reta que procuramos é\n\\[\n\\hat y = 1{,}00 + 2{,}00 x\n\\]\nNo gráfico, os valores de \\(\\hat y\\), para cada valor de \\(x\\), são mostrados em vermelho. A reta de regressão é mostrada em azul:\n\n\n\n\n\n\n\n\nOs valores de \\(y\\), os valores previstos e os resíduos são\n\n\n\n\n\n\n\nx\ny\nprevisto\nresíduo\n\n\n\n1\n4\n3\n1\n\n\n2\n3\n5\n-2\n\n\n3\n8\n7\n1\n\n\n\n\n\n\n\nUsando Álgebra Linear, vamos encarar este modelo de outra forma.\nA coluna y dos dados é representada pelo vetor\n\\[\n\\mathbf{Y} = \\begin{bmatrix}\n  4 \\\\ 3 \\\\ 8\n\\end{bmatrix}\n\\]\nVamos definir a seguinte matriz:\n\\[\n\\mathbf{X} = \\begin{bmatrix}\n  1 & 1 \\\\ 1 & 2 \\\\ 1 & 3\n\\end{bmatrix}\n\\]\nNesta matriz, a segunda coluna corresponde à coluna x dos dados. A primeira coluna, com valores \\(1\\), está ali para podermos escrever o modelo como a equação matricial\n\\[\n\\mathbf{\\widehat{Y}} = \\mathbf{X} \\cdot \\begin{bmatrix}\n  \\beta_0 \\\\ \\beta_1\n\\end{bmatrix}\n\\]\nque, de forma mais detalhada, é\n\\[\n\\begin{bmatrix}\n  \\widehat{y_1} \\\\ \\widehat{y_2} \\\\ \\widehat{y_3}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  1 & 1 \\\\ 1 & 2 \\\\ 1 & 3\n\\end{bmatrix}\n\\cdot \\begin{bmatrix}\n  \\beta_0 \\\\ \\beta_1\n\\end{bmatrix}\n\\]\nou, ainda,\n\\[\n\\begin{bmatrix}\n  \\widehat{y_1} \\\\ \\widehat{y_2} \\\\ \\widehat{y_3}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n  \\beta_0 + \\phantom{1\\cdot{}}\\beta_1 \\\\\n  \\beta_0 + 2\\cdot \\beta_1 \\\\\n  \\beta_0 + 3\\cdot \\beta_1\n\\end{bmatrix}\n\\]\nou, explicitando os vetores que correspondem às colunas da matriz \\(\\mathbf{X}\\):\n\\[\n\\begin{bmatrix}\n  \\widehat{y_1} \\\\ \\widehat{y_2} \\\\ \\widehat{y_3}\n\\end{bmatrix}\n=\n\\beta_0 \\cdot\n\\begin{bmatrix}\n  1 \\\\ 1 \\\\ 1\n\\end{bmatrix}\n+\n\\beta_1 \\cdot\n\\begin{bmatrix}\n  1 \\\\ 2 \\\\ 3\n\\end{bmatrix}\n\\tag{1.1}\\]\nAgora, as considerações geométricas:\n\nAs colunas x e y do conjunto de dados são vetores com \\(3\\) componentes, que vivem em \\(\\mathbb{R}^3\\).\nO vetor \\(\\mathbf{\\widehat Y}\\) também tem \\(3\\) componentes, mas a Equação 1.1 está dizendo que \\(\\mathbf{\\widehat Y}\\) é uma combinação linear dos dois vetores (linearmente independentes) \\([1\\ 1\\ 1]^T\\) e \\([1\\ 2\\ 3]^T\\).\nOs dois vetores \\([1\\ 1\\ 1]^T\\) e \\([1\\ 2\\ 3]^T\\) não são capazes de gerar todo o espaço \\(\\mathbb{R}^3\\); o espaço gerado por eles é um plano.\nO vetor \\(\\mathbf{Y}\\) (com os valores verdadeiros da variável de resposta \\(y\\)) não está no plano gerado pelos vetores \\([1\\ 1\\ 1]^T\\) e \\([1\\ 2\\ 3]^T\\) (verifique).\n\nA relação verdadeira entre \\(\\mathbf{Y}\\) e \\(\\mathbf{X}\\) é\n\\[\n\\begin{bmatrix}\n   y_1 \\\\ {y_2} \\\\ {y_3}\n\\end{bmatrix}\n=\n\\beta_0 \\cdot\n\\begin{bmatrix}\n   1 \\\\ 1 \\\\ 1\n\\end{bmatrix}\n+\n\\beta_1 \\cdot\n\\begin{bmatrix}\n   1 \\\\ 2 \\\\ 3\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n   \\varepsilon_1 \\\\ \\varepsilon_2 \\\\ \\varepsilon_3\n\\end{bmatrix}\n\\] onde os valores \\(\\varepsilon_i\\) são os erros que o modelo não consegue capturar.\n\n\nEstes erros \\(\\varepsilon_i\\) são estimados pelos resíduos \\(\\widehat{\\varepsilon_i}\\), de maneira que podemos escrever\n\\[\n\\begin{bmatrix}\n   \\widehat{y_1} \\\\ \\widehat{y_2} \\\\ \\widehat{y_3}\n\\end{bmatrix}\n=\n\\beta_0 \\cdot\n\\begin{bmatrix}\n   1 \\\\ 1 \\\\ 1\n\\end{bmatrix}\n+\n\\beta_1 \\cdot\n\\begin{bmatrix}\n   1 \\\\ 2 \\\\ 3\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n   \\widehat{\\varepsilon_1} \\\\\n   \\widehat{\\varepsilon_2} \\\\\n   \\widehat{\\varepsilon_3}\n\\end{bmatrix}\n\\]\nO vetor de resíduos é\n\\[\n\\mathbf{\\widehat{\\varepsilon}}\n=\n\\begin{bmatrix}\n   \\widehat{\\varepsilon_1} \\\\\n   \\widehat{\\varepsilon_2} \\\\\n   \\widehat{\\varepsilon_3}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    \\phantom{-}1 \\\\ -2 \\\\ \\phantom{-}1\n\\end{bmatrix}\n\\]\n\n\nA situação é mostrada na figura:\n\nO plano cinza é o espaço gerado pelos vetores \\([1\\ 1\\ 1]^T\\) e \\([1\\ 2\\ 3]^T\\). Na equação paramétrica deste plano, \\(r\\) e \\(s\\) correspondem aos valores possíveis de \\(\\beta_0\\) e \\(\\beta_1\\), respectivamente.\nO vetor \\(\\mathbf{\\widehat{Y}}\\) (dos valores previstos pelo modelo) é a projeção ortogonal do vetor \\(\\mathbf{Y}\\) (dos valores verdadeiros da variável de resposta) sobre o plano gerado pelas colunas da matriz \\(\\mathbf{X}\\). Mais abaixo, vamos ver os detalhes desta projeção. O importante é entender que, quaisquer que sejam os valores de \\(\\beta_0\\) e \\(\\beta_1\\), o vetor \\(\\mathbf{\\widehat{Y}}\\) de valores previstos vai estar sempre limitado ao plano gerado pelas colunas da matriz \\(\\mathbf{X}\\).\nIsto corresponde à intuição de que estamos perdendo informação ao tentar representar objetos de dimensão \\(3\\) (o número de observações do conjunto de dados) em um espaço de dimensão \\(2\\) (o número de parâmetros do modelo: \\(\\beta_0\\) e \\(\\beta_1\\)).\n???\n\n\n\n\n\n\nDe Veaux, R. D., P. F. Velleman, e D. E. Bock. 2016. Stats: Data and Models. 4.ª ed. Pearson Education. https://media.pearsoncmg.com/aw/aw_deveaux_stats_4_2016/websites/statdm4d_comp_web_launch.html.\n\n\nFaraway, Julian J. 2016. Linear Models with R. 2.ª ed. Chapman; Hall/CRC. https://doi.org/10.1201/b17144.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, e Robert Tibshirani. 2021. An Introduction to Statistical Learning: With Applications in R. 2.ª ed. Springer Publishing Company, Incorporated. https://www.statlearning.com/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regressão linear simples</span>"
    ]
  },
  {
    "objectID": "multipla.html",
    "href": "multipla.html",
    "title": "\n2  Regressão linear múltipla\n",
    "section": "",
    "text": "2.1 Simulação",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão linear múltipla</span>"
    ]
  },
  {
    "objectID": "multipla.html#simulação",
    "href": "multipla.html#simulação",
    "title": "\n2  Regressão linear múltipla\n",
    "section": "",
    "text": "Multicolinearidade\nVamos criar três preditores x1, x2 e x3, com os dois primeiros correlacionados:\n\nn &lt;- 100\na &lt;- 2\nx1 &lt;- runif(n)\nx2 &lt;- a * x1 + rnorm(n, 0, .1)\nx3 &lt;- runif(n)\n\ndf &lt;- tibble(x1, x2, x3)\n\nGráficos:\n\nplot_cor &lt;- function(df, v1, v2) {\n  \n  x = df[[v1]]\n  y = df[[v2]]\n  valor_cor &lt;- cor(x, y) %&gt;% round(4)\n  \n  df %&gt;% ggplot(aes(x, y)) +\n    geom_point(alpha = 0.5) +\n    labs(\n      title = paste0('cor(', v1, ', ', v2, ') = ', valor_cor),\n      x = v1,\n      y = v2\n    )\n  \n}\n\n\nv &lt;- c('x1', 'x2', 'x3')\n\npares &lt;- expand_grid(x = v, y = v) %&gt;% \n  filter(x &lt; y) %&gt;% \n  arrange(x, y)\n\nv1 &lt;- pares %&gt;% pull(x)\nv2 &lt;- pares %&gt;% pull(y)\n\nplots &lt;- map2(\n  v1, v2, ~ plot_cor(df, .x, .y)\n)\n\nplots %&gt;% \n  wrap_plots(\n    ncol = 1,\n    byrow = TRUE\n  )\n\n\n\n\n\n\n\nA variável de resposta é y:\n\nb0 &lt;- 1\nb1 &lt;- 2\nb2 &lt;- 3\nb3 &lt;- 4\nvar_epsilon &lt;- .5\n\ny &lt;- b0 + b1 * x1 + b2 * x2 + b3 * x3 + rnorm(n, sd = sqrt(var_epsilon))\ndf_y &lt;- df %&gt;% \n  mutate(y = y)\n\nUsando todas as variáveis, temos:\n\nmodelo_123 &lt;- lm(y ~ ., data = df_y)\n\nA equação verdadeira é\n\\[\ny = 1 + 2 x_1 + 3 x_2 + 4 x_3 + \\varepsilon\n\\]\nO modelo deu os coeficientes\n\nmodelo_123 %&gt;% summary()\n\n\nCall:\nlm(formula = y ~ ., data = df_y)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2,39333 -0,42820  0,08625  0,38997  1,83332 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   1,2345     0,1747   7,068       0,000000000252 ***\nx1            3,2787     1,4582   2,249              0,02683 *  \nx2            2,3932     0,7203   3,323              0,00126 ** \nx3            3,7137     0,2286  16,245 &lt; 0,0000000000000002 ***\n---\nSignif. codes:  0 '***' 0,001 '**' 0,01 '*' 0,05 '.' 0,1 ' ' 1\n\nResidual standard error: 0,681 on 96 degrees of freedom\nMultiple R-squared:  0,9334,    Adjusted R-squared:  0,9313 \nF-statistic: 448,6 on 3 and 96 DF,  p-value: &lt; 0,00000000000000022\n\nmodelo_123\n\n\nCall:\nlm(formula = y ~ ., data = df_y)\n\nCoefficients:\n(Intercept)           x1           x2           x3  \n      1,235        3,279        2,393        3,714  \n\n\nAgora, usando apenas x1 e x3:\n\nmodelo_13 &lt;- lm(y ~ x1 + x3, data = df_y)\n\nA equação verdadeira é — substituindo \\(x_2\\) por \\((b_1 + ab_2)x_1\\) —\n\\[\ny = 1 + 8 x_1 + 4 x_3 + \\varepsilon\n\\]\nO modelo deu os coeficientes\n\nmodelo_13 %&gt;% summary()\n\n\nCall:\nlm(formula = y ~ x1 + x3, data = df_y)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2,2807 -0,4648  0,0890  0,4492  1,8246 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)   1,2489     0,1834   6,808       0,000000000825 ***\nx1            8,0547     0,2580  31,224 &lt; 0,0000000000000002 ***\nx3            3,7315     0,2401  15,543 &lt; 0,0000000000000002 ***\n---\nSignif. codes:  0 '***' 0,001 '**' 0,01 '*' 0,05 '.' 0,1 ' ' 1\n\nResidual standard error: 0,7153 on 97 degrees of freedom\nMultiple R-squared:  0,9258,    Adjusted R-squared:  0,9242 \nF-statistic: 604,7 on 2 and 97 DF,  p-value: &lt; 0,00000000000000022\n\nmodelo_13\n\n\nCall:\nlm(formula = y ~ x1 + x3, data = df_y)\n\nCoefficients:\n(Intercept)           x1           x3  \n      1,249        8,055        3,732  \n\n\nEm termos do \\(R^2\\) ajustado:\n\nO modelo com os três preditores teve \\(R^2_{\\text{adj}} = 0{,}9313\\).\nO modelo com dois preditores teve \\(R^2_{\\text{adj}} = 0{,}9242\\).\n\nPara a equação verdadeira:\n\ny_eq &lt;- b0 + b1 * x1 + b2 * x2 + b3 * x3\nrsq_vec(y, y_eq)\n\n[1] 0,931878\n\n\nAnova diz que o segundo modelo é mais significativo que o primeiro:\n\nanova(modelo_123, modelo_13)\n\n\n  \n\n\n\n\nmodelo_123 %&gt;% glance()\n\n\n  \n\n\n\n\nmodelo_13 %&gt;% glance()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regressão linear múltipla</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referências",
    "section": "",
    "text": "De Veaux, R. D., P. F. Velleman, e D. E. Bock. 2016. Stats: Data and\nModels. 4.ª ed. Pearson Education. https://media.pearsoncmg.com/aw/aw_deveaux_stats_4_2016/websites/statdm4d_comp_web_launch.html.\n\n\nFaraway, Julian J. 2016. Linear Models with R. 2.ª ed. Chapman;\nHall/CRC. https://doi.org/10.1201/b17144.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, e Robert Tibshirani. 2021.\nAn Introduction to Statistical Learning: With Applications in\nR. 2.ª ed. Springer Publishing Company, Incorporated. https://www.statlearning.com/.",
    "crumbs": [
      "Referências"
    ]
  }
]